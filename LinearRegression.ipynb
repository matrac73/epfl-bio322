{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from module import NaN_checker, standardizer, NaN_remover, extract_features_from_mol, unbiased_RT, features_reduction_using_correlation, correlation_matrix, enrich, PrincipalComponentAnalysis\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_scaled.csv')\n",
    "enriched_train = enrich(pd.read_csv('train.csv'))\n",
    "train_cddd_merged = pd.read_csv('train_cddd_merged.csv')\n",
    "cddd = pd.read_csv('cddd_scaled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to perform linear regression on our datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearregression(xtrain, ytrain, xtest, ytest = pd.DataFrame()): \n",
    "    \"\"\"\n",
    "    performing linear regression on a Dataframe. \n",
    "    \n",
    "    Parameters : \n",
    "    - xtrain (pd.dataframe) : Dataframe containing features values to train the model on.\n",
    "    - ytrain (pd.dataframe) : Dataframe containing outcome variable values to train de model on.\n",
    "    - xtest (pd.dataframe) : Dataframe containing features values to test the model on. \n",
    "    - ytest (pd.dataframe) : Datadrame containing outcome variable values to tesz the model on.\n",
    "    \n",
    "    Returns : \n",
    "    - train_mse : mean squared error between the outcome values estimated by the model and the real outcome values in the training set (ytrain).\n",
    "    - test_mse : mean squared error between the outcome values estimated by the model and the real outcome values in the test set (ytest).\n",
    "    - coefs : Dataframe containing the coefficients found by the model for each feature. \n",
    "    - RT : numpy.ndarray of the values predicted by the model.\n",
    "    \"\"\"\n",
    "    m = LinearRegression()\n",
    "    m.fit(xtrain,ytrain)\n",
    "    coefs = pd.DataFrame({\n",
    "\t    'predictor': xtrain.columns.tolist(),\n",
    "\t    'value': m.coef_[0]\n",
    "\t    })\n",
    "    RT = m.predict(xtest)\n",
    "    train_mse = mean_squared_error(m.predict(xtrain), ytrain) ** 0.5\n",
    "    if ytest.empty: \n",
    "        test_mse = None\n",
    "    else : \n",
    "        test_mse = mean_squared_error(m.predict(xtest), ytest) ** 0.5\n",
    "    return train_mse, test_mse, coefs, RT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the function to our four different datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6944561240305511\n",
      "32648251296803.21\n",
      "[-1.18398326e+14  5.55139160e+00  1.06051025e+01 ...  1.29775391e+01\n",
      " -1.89780498e+13  7.07922363e+00]\n"
     ]
    }
   ],
   "source": [
    "xtrain = train.iloc[:1000].drop(['Compound', 'SMILES', 'Lab', 'mol','RT'], axis=1)\n",
    "ytrain = train['RT'].iloc[:1000]\n",
    "xtest = train.iloc[1001:].drop(['Compound', 'SMILES', 'Lab', 'mol','RT'], axis=1)\n",
    "ytest = train['RT'].iloc[1001:]\n",
    "\n",
    "train_mse, test_mse, coefs, RT = linearregression(xtrain, ytrain, xtest, ytest)\n",
    "print(train_mse)\n",
    "print(test_mse)\n",
    "print(RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6801276862081973\n",
      "1570336196062.9834\n",
      "[ 1.34727478e+01  4.90044403e+00 -9.54943992e+09 ...  1.91985497e+10\n",
      "  2.74422892e+12  5.53390503e+00]\n"
     ]
    }
   ],
   "source": [
    "xtrain = enriched_train.iloc[:1000].drop(['Compound', 'SMILES', 'Lab','RT', 'mean_RT', 'Bias', 'Corrected_RT'], axis=1)\n",
    "ytrain = enriched_train['RT'].iloc[:1000]\n",
    "xtest = enriched_train.iloc[1001:].drop(['Compound', 'SMILES', 'Lab','RT', 'mean_RT', 'Bias','Corrected_RT'], axis=1)\n",
    "ytest = enriched_train['RT'].iloc[1001:]\n",
    "\n",
    "train_mse, test_mse, coefs, RT = linearregression(xtrain, ytrain, xtest, ytest)\n",
    "print(train_mse)\n",
    "print(test_mse)\n",
    "print(RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5404516607900411\n",
      "27543149063608.527\n",
      "[ 1.03959961e+01  6.19396973e+00  3.62496319e+10 ...  2.50203437e+12\n",
      " -4.54277372e+13  3.25769043e+00]\n"
     ]
    }
   ],
   "source": [
    "xtrain = train_cddd_merged.iloc[:1000].drop(['Compound', 'SMILES', 'Lab', 'mol','RT'], axis=1)\n",
    "ytrain = train_cddd_merged['RT'].iloc[:1000]\n",
    "xtest = train_cddd_merged.iloc[1001:].drop(['Compound', 'SMILES', 'Lab', 'mol','RT'], axis=1)\n",
    "ytest = train_cddd_merged['RT'].iloc[1001:]\n",
    "\n",
    "train_mse, test_mse, coefs, RT = linearregression(xtrain, ytrain, xtest, ytest)\n",
    "print(train_mse)\n",
    "print(test_mse)\n",
    "print(RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.688987202033922\n",
      "115513721801.30844\n",
      "[-1.00235246e+11  5.65585327e+00  1.06555252e+01 ...  1.27994690e+01\n",
      "  3.80443493e+11  7.08302307e+00]\n"
     ]
    }
   ],
   "source": [
    "withoutcddd = pd.read_csv('train_scaled.csv')\n",
    "withoutcddd = extract_features_from_mol(withoutcddd)\n",
    "xtrain = withoutcddd.iloc[:1000].drop(['Compound', 'SMILES', 'Lab','RT'], axis=1)\n",
    "ytrain = withoutcddd['RT'].iloc[:1000]\n",
    "xtest = withoutcddd.iloc[1001:].drop(['Compound', 'SMILES', 'Lab','RT'], axis=1)\n",
    "ytest = withoutcddd['RT'].iloc[1001:]\n",
    "\n",
    "train_mse, test_mse, coefs, RT = linearregression(xtrain, ytrain, xtest, ytest)\n",
    "print(train_mse)\n",
    "print(test_mse)\n",
    "print(RT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a huge test mean squared error. It probably due to the huge number of features that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best combination of parameters\n",
    "We want to find the best combination of parameters to have the smaller test mean squared error. \n",
    "First, we define a function to evaluate the mean squared error for a list of given predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(train, test, predictor_names, target = 'RT'):\n",
    "    \"\"\"\n",
    "    Evaluate the test mean squared error of a train and a test Dataframes for a given list of predictors\n",
    "    \n",
    "    Parameters : \n",
    "    - train (pd.dataframe) : Dataframe containing the training data for the linear model.\n",
    "    - test (pd.dataframe) : Dataframe containing the test data which will serve to estimate the mean squared error of the model. \n",
    "    - perdictor_names (pd.dataframe) : Dataframe containing a list of the predictors used by the linear model.\n",
    "    \n",
    "    Returns : \n",
    "    - np.array : containing the mean squared error for the prediction with the predictors given.\n",
    "    \"\"\"\n",
    "    model = LinearRegression().fit(train[predictor_names].values,\n",
    "                                   train[target].values)\n",
    "    y_pred = model.predict(test[predictor_names].values)\n",
    "    return np.sqrt(mean_squared_error(test[target].values, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a function that returns the names of the best predictors to have the smalest test mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_predictors(train, test, target = ['RT']) : \n",
    "    \"\"\"\n",
    "    Find the best combination of two predictors to predict the target value in a Dataframe.\n",
    "    \n",
    "    Parameters : \n",
    "    - train (pd.dataframe) : Dataframe containing the training data for the linear model.\n",
    "    - test (pd.dataframe) : Dataframe containing the test data which will serve to estimate the mean squared error of the model. \n",
    "    - target : the target value we are trying to predict with the given predictors. \n",
    "    \n",
    "    Returns : \n",
    "    - pd.dataframe : containing the mean squared error of each pairs of predictors.\n",
    "    \"\"\"\n",
    "    predictors = [col for col in train.columns if col not in target]\n",
    "    predictor_pairs = [[p1, p2] for p1 in predictors for p2 in predictors if p1 > p2]\n",
    "    results = [(run_and_evaluate(train, test, p), \", \".join(p)) for p in predictor_pairs]\n",
    "    results_df = pd.DataFrame(results, columns=[\"test_loss\", \"predictors\"]).sort_values(by=\"test_loss\").reset_index(drop=True)\n",
    "    return results_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply our function to the enriched Dataframe. (result : ECFP_888 and ECFP_334 not really reliable as overfitting of the sample maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\annab\\EPFL\\Machine Learning\\BIO-322\\LinearRegression.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train \u001b[39m=\u001b[39m enriched_train\u001b[39m.\u001b[39miloc[:\u001b[39m1000\u001b[39m]\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mCompound\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSMILES\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLab\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmean_RT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCorrected_RT\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test \u001b[39m=\u001b[39m enriched_train\u001b[39m.\u001b[39miloc[\u001b[39m1001\u001b[39m:]\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mCompound\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSMILES\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLab\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmean_RT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBias\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mCorrected_RT\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m bp \u001b[39m=\u001b[39m best_predictors(train, test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(bp)\n",
      "\u001b[1;32mc:\\Users\\annab\\EPFL\\Machine Learning\\BIO-322\\LinearRegression.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m predictors \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m train\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m target]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m predictor_pairs \u001b[39m=\u001b[39m [[p1, p2] \u001b[39mfor\u001b[39;00m p1 \u001b[39min\u001b[39;00m predictors \u001b[39mfor\u001b[39;00m p2 \u001b[39min\u001b[39;00m predictors \u001b[39mif\u001b[39;00m p1 \u001b[39m>\u001b[39m p2]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m results \u001b[39m=\u001b[39m [(run_and_evaluate(train, test, p), \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(p)) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m predictor_pairs]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpredictors\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m results_df\n",
      "\u001b[1;32mc:\\Users\\annab\\EPFL\\Machine Learning\\BIO-322\\LinearRegression.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m predictors \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m train\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m target]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m predictor_pairs \u001b[39m=\u001b[39m [[p1, p2] \u001b[39mfor\u001b[39;00m p1 \u001b[39min\u001b[39;00m predictors \u001b[39mfor\u001b[39;00m p2 \u001b[39min\u001b[39;00m predictors \u001b[39mif\u001b[39;00m p1 \u001b[39m>\u001b[39m p2]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m results \u001b[39m=\u001b[39m [(run_and_evaluate(train, test, p), \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(p)) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m predictor_pairs]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results, columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpredictors\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m results_df\n",
      "\u001b[1;32mc:\\Users\\annab\\EPFL\\Machine Learning\\BIO-322\\LinearRegression.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mEvaluate the test mean squared error of a train and a test Dataframes for a given list of predictors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m- np.array : containing the mean squared error for the prediction with the predictors given.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m LinearRegression()\u001b[39m.\u001b[39mfit(train[predictor_names]\u001b[39m.\u001b[39mvalues,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                train[target]\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(test[predictor_names]\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/annab/EPFL/Machine%20Learning/BIO-322/LinearRegression.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msqrt(mean_squared_error(test[target]\u001b[39m.\u001b[39mvalues, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\annab\\anaconda3\\envs\\MLCourse\\lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training = enriched_train.iloc[:1000].drop(['Compound', 'SMILES', 'Lab', 'mean_RT', 'Bias', 'Corrected_RT'], axis=1)\n",
    "testing = enriched_train.iloc[1001:].drop(['Compound', 'SMILES', 'Lab', 'mean_RT', 'Bias','Corrected_RT'], axis=1)\n",
    "\n",
    "bp = best_predictors(train, test)\n",
    "\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We naively try to do a linear regression with the features appearing high in the classement to see if it improves the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7858833887296208\n",
      "1.7944313743025826\n",
      "[12.98461199  5.95069099  3.32716166 ...  9.98875197  6.82005831\n",
      "  5.3973023 ]\n"
     ]
    }
   ],
   "source": [
    "xtrain = enriched_train[['cddd_432', 'cddd_458', 'CarbonAtoms', 'TotalAtoms', 'MolLogP', 'lab_mean_bias']].iloc[:1000]\n",
    "ytrain = enriched_train['RT'].iloc[:1000]\n",
    "xtest = enriched_train[['cddd_432', 'cddd_458', 'CarbonAtoms', 'TotalAtoms', 'MolLogP', 'lab_mean_bias']].iloc[1001:]\n",
    "ytest = enriched_train['RT'].iloc[1001:]\n",
    "\n",
    "train_mse, test_mse, coefs, RT = linearregression(xtrain, ytrain, xtest, ytest)\n",
    "print(train_mse)\n",
    "print(test_mse)\n",
    "print(RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean squared error is clearly better but also because we have less parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing features using Principal Component Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 35)\n",
      "Index(['Compound', 'SMILES', 'Lab', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6',\n",
      "       'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15',\n",
      "       'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24',\n",
      "       'PC25', 'PC26', 'PC27', 'PC28', 'PC29', 'PC30', 'PC31', 'RT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "PCA_train = PrincipalComponentAnalysis(enriched_train, 31)\n",
    "print(PCA_train.shape)\n",
    "print(PCA_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to do a linear regression with the obtained dataset after Principale Component Analysis (keeping percentage = 31%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8934905906695358\n",
      "1.9897580765967073\n",
      "[12.98986074  5.36538892  5.22047815 ... 10.34828786  6.49447991\n",
      "  3.98856037]\n"
     ]
    }
   ],
   "source": [
    "xtrain = PCA_train.drop(['Compound', 'SMILES', 'Lab','RT'], axis = 1).iloc[:1000]\n",
    "ytrain = PCA_train['RT'].iloc[:1000]\n",
    "xtest = PCA_train.drop(['Compound', 'SMILES', 'Lab', 'RT'], axis = 1).iloc[1001:]\n",
    "ytest = PCA_train['RT'].iloc[1001:]\n",
    "\n",
    "train_mse, test_mse, coefs, RT = linearregression(xtrain, ytrain, xtest, ytest)\n",
    "print(train_mse)\n",
    "print(test_mse)\n",
    "print(RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an improvement but not as good as when we used naively the predictors in the best pairs of predictors (maybe because we have more predictors (we could try with a lower keeping percentage))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to try the last non zero features when performing Lasso regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5679599423272075\n",
      "2.616480099093414\n",
      "[ 8.35390613  6.87708716  7.85501341 ... 10.83102655  5.32727484\n",
      "  4.68512673]\n"
     ]
    }
   ],
   "source": [
    "xtrain = enriched_train[['ECFP_41','ECFP_46','ECFP_152','ECFP_262','ECFP_334','ECFP_491','ECFP_550','ECFP_592','ECFP_882','ECFP_888',\n",
    "                        'ECFP_990', 'MolecularWeight','TotalAtoms','CarbonAtoms','MolLogP','cddd_9','cddd_14','cddd_27','cddd_52','cddd_74',\n",
    "                        'cddd_159','cddd_237','cddd_251','cddd_370','cddd_374','cddd_380','cddd_410','cddd_450', 'cddd_508']].iloc[:1000]\n",
    "ytrain = enriched_train['RT'].iloc[:1000]\n",
    "xtest = enriched_train[['ECFP_41','ECFP_46','ECFP_152','ECFP_262','ECFP_334','ECFP_491','ECFP_550','ECFP_592','ECFP_882','ECFP_888',\n",
    "                        'ECFP_990', 'MolecularWeight','TotalAtoms','CarbonAtoms','MolLogP','cddd_9','cddd_14','cddd_27','cddd_52','cddd_74',\n",
    "                        'cddd_159','cddd_237','cddd_251','cddd_370','cddd_374','cddd_380','cddd_410','cddd_450', 'cddd_508']].iloc[1001:]\n",
    "ytest = enriched_train['RT'].iloc[1001:]\n",
    "\n",
    "train_mse, test_mse, coefs, RT = linearregression(xtrain, ytrain, xtest, ytest)\n",
    "print(train_mse)\n",
    "print(test_mse)\n",
    "print(RT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA gives better results so we are going to stick to that for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try the linear regression combined with a lasso regularisation. We found in the regularisation notebook that, when using the enriched train dataset, the best alpha was 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 6.990276765924979\n",
      "Nombre de coefficients non nuls : 89\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(enriched_train.drop(['Compound', 'SMILES', 'Lab','RT'], axis = 1), enriched_train['RT'], test_size=0.2, random_state=42)\n",
    "\n",
    "alpha = 0.1  \n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = lasso_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "non_zero_coefficients = np.sum(lasso_model.coef_ != 0)\n",
    "print(f'Nombre de coefficients non nuls : {non_zero_coefficients}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to combine the lasso regularisation with the PCA features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.5301135523902065\n",
      "Nombre de coefficients non nuls : 30\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(PCA_train.drop(['Compound', 'SMILES', 'Lab','RT'], axis = 1), PCA_train['RT'], test_size=0.2, random_state=42)\n",
    "\n",
    "alpha = 0.1  \n",
    "lasso_model = Lasso(alpha=alpha)\n",
    "\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = lasso_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "non_zero_coefficients = np.sum(lasso_model.coef_ != 0)\n",
    "print(f'Nombre de coefficients non nuls : {non_zero_coefficients}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic linear regression kaggle submission\n",
    "(We have seen that, strangely, if we added by mistake a second, not scaled cddd to the enriched train data set, the mean squared error was even lower.) To be able to submit our estimations to kaggle, we have to prepare our test set the same way our training set (enriched train) was prepared. Using the enrich function. The fonction will extract features from the mol column, add the cddd features and replace the NaN in the lines where no cddd was given (no corresponding SMILES) by the mean of their column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Compound  \\\n",
      "0               5Cl-AB-PINACA   \n",
      "1                  SDB-006-5F   \n",
      "2                  Cephaeline   \n",
      "3     JWH-081 (4-OH-Naphthyl)   \n",
      "4                      5F-AEB   \n",
      "...                       ...   \n",
      "1370      4-methylamphetamine   \n",
      "1371                  Cocaine   \n",
      "1372               Clonazolam   \n",
      "1373       1-Hydroxymidazolam   \n",
      "1374            Phenylephrine   \n",
      "\n",
      "                                                 SMILES  \\\n",
      "0            CC(C)C(NC(=O)c1nn(CCCCCCl)c2ccccc12)C(N)=O   \n",
      "1                  O=C(NCc1ccccc1)c1cn(CCCCCF)c2ccccc12   \n",
      "2     CCC1CN2CCc3cc(OC)c(OC)cc3C2CC1CC1NCCc2cc(O)c(O...   \n",
      "3            CCCCCn1cc(C(=O)c2ccc(O)c3ccccc23)c2ccccc21   \n",
      "4           CCOC(=O)C(NC(=O)c1nn(CCCCCF)c2ccccc12)C(C)C   \n",
      "...                                                 ...   \n",
      "1370                                  Cc1ccc(CC(C)N)cc1   \n",
      "1371               COC(=O)C1C(OC(=O)c2ccccc2)CC2CCC1N2C   \n",
      "1372  Cc1nnc2n1-c1ccc([N+](=O)[O-])cc1C(c1ccccc1Cl)=NC2   \n",
      "1373            OCc1ncc2n1-c1ccc(Cl)cc1C(c1ccccc1F)=NC2   \n",
      "1374                                 CNCC(O)c1cccc(O)c1   \n",
      "\n",
      "                                           Lab  \\\n",
      "0                                        CFSRE   \n",
      "1                         University of Athens   \n",
      "2                                        CFSRE   \n",
      "3                                        Mainz   \n",
      "4        Zurich Institute of Forensic Medicine   \n",
      "...                                        ...   \n",
      "1370                                Copenhagen   \n",
      "1371                          Ghent University   \n",
      "1372  Victorian Institute of Forensic Medicine   \n",
      "1373                                     CFSRE   \n",
      "1374  Victorian Institute of Forensic Medicine   \n",
      "\n",
      "                                                    mol  ECFP_1  ECFP_2  \\\n",
      "0     <rdkit.Chem.rdchem.Mol object at 0x000001C500E...       0       1   \n",
      "1     <rdkit.Chem.rdchem.Mol object at 0x000001C500E...       0       0   \n",
      "2     <rdkit.Chem.rdchem.Mol object at 0x000001C500E...       0       0   \n",
      "3     <rdkit.Chem.rdchem.Mol object at 0x000001C500E...       0       0   \n",
      "4     <rdkit.Chem.rdchem.Mol object at 0x000001C500E...       0       1   \n",
      "...                                                 ...     ...     ...   \n",
      "1370  <rdkit.Chem.rdchem.Mol object at 0x000001C500D...       0       1   \n",
      "1371  <rdkit.Chem.rdchem.Mol object at 0x000001C500D...       0       1   \n",
      "1372  <rdkit.Chem.rdchem.Mol object at 0x000001C500D...       0       1   \n",
      "1373  <rdkit.Chem.rdchem.Mol object at 0x000001C500D...       0       0   \n",
      "1374  <rdkit.Chem.rdchem.Mol object at 0x000001C500D...       0       1   \n",
      "\n",
      "      ECFP_3  ECFP_4  ECFP_5  ECFP_6  ...  cddd_503  cddd_504  cddd_505  \\\n",
      "0          0       0       0       0  ...  1.898016 -0.251531  0.603756   \n",
      "1          0       0       0       0  ...  1.597593 -1.237748 -1.117227   \n",
      "2          0       1       0       0  ... -0.798638  0.696439  1.407564   \n",
      "3          0       0       0       0  ... -1.152707  0.305204  0.961351   \n",
      "4          0       0       0       0  ...  0.820909 -0.421866 -1.990155   \n",
      "...      ...     ...     ...     ...  ...       ...       ...       ...   \n",
      "1370       0       0       0       0  ...  0.335806  1.175263  1.339117   \n",
      "1371       0       0       0       0  ... -1.869848 -1.063020 -1.853934   \n",
      "1372       0       0       0       0  ...  0.431747  0.678571  1.017432   \n",
      "1373       0       0       0       0  ...  1.121303  0.521605 -0.303217   \n",
      "1374       0       0       0       0  ... -0.882772  1.546477  0.438011   \n",
      "\n",
      "      cddd_506  cddd_507  cddd_508  cddd_509  cddd_510  cddd_511  cddd_512  \n",
      "0    -0.839419 -0.433324 -1.259263 -1.402565  0.206017  1.338716 -0.145712  \n",
      "1     0.652552 -0.421927  1.409858  1.027226  0.698305  0.506029  1.659830  \n",
      "2     1.296428 -0.435698  0.189608  1.040978  0.699162 -0.177902  0.103966  \n",
      "3    -0.606387 -0.429850 -1.343259  0.466784  0.094919  0.047730 -1.355922  \n",
      "4     0.768118 -0.434529 -1.546046 -1.079567  1.298422  0.468079 -0.427472  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1370 -0.037874  4.276618  0.012613 -0.403340 -1.403525  1.237485  0.132374  \n",
      "1371  0.949525 -0.427186  1.102536 -1.041237  0.386467  0.320679  0.189058  \n",
      "1372  0.346696 -0.430232 -0.172349 -0.511533 -0.316588 -1.651293  0.761454  \n",
      "1373  1.322681 -0.429300  1.016582  1.154704  0.629858 -1.952104  1.317458  \n",
      "1374  0.382078  3.000631  0.077234 -1.117243 -2.264342 -0.138100 -0.748835  \n",
      "\n",
      "[1375 rows x 1540 columns]\n"
     ]
    }
   ],
   "source": [
    "enriched_test = enrich(test)\n",
    "print(enriched_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to add the laboratory bias to our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our test set is ready, we are going to have to train our model on the full training set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
